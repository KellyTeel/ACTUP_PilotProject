# Workflow


The overall process of our project was a long series of trails and errors, as is most projects in the field of Digital Humanities. Through this process we encountered many successes, but even more failures. In general, both Catherine and Kelly worked together to create various visualizations with the data provided by David Thomas. Indiivudally they analyzed these visualizations, coming to a general conclusion about the data from the [ACTUP Oral History Project](http://actuporalhistory.org/index1.html).

---

As stated previously, when deciding on a topic to persure for our  Pilot Project, we both agreed that our mutal interest in LGBTQ+ history would be the focal point for thier search. Kelly discovered the ACTUP Oral History Project after many hours serarching for a historical database realted to LGBTQ+ studies. With limited options available, and the genuie interest we had in ACTUP, we decided to attempt to  analyze this source for our prject.

[David Thomas](http://theportus.com) provided us with txt files of oral history transcripts from ACT UP's online oral history repository. This was completed via a [webscarape](http://webscraper.io/) accomplished in 2017. To complete this  webscrape normally, we would have had to open all 186 PDF files at once and scrape each document individually. Professor Thomas was kind enough to save us the time and headache by providing us the data from his webscrape.

![image](imgs/w1.PNG)

![image](imgs/w2.PNG)

From those txt files we were able to upload them into Orange Three and Voyant. Catherine tackled Orange 3, while Kelly used Voyant. 

In [Voyant](http://voyant-tools.org/), Kelly tried various different configurations and visualizations to extract information from the collection of transcripts. Unlike with Orange 3, there wasn't any obvious way to add stop-words to the data, meaning that appreviations like "SS" and common, formal words such as "like" were included in word  counts and links, leading to  some less than useful visualizations. None the less, she still managed to produce a couple very helpful tables and visuals, as seen in the "Inital Findings" page.

![image](imgs/v4.PNG)

![image](imgs/v5.PNG)

The data derived was successfully uploaded into Tableau and was analyzed using a sentiment analysis tool. Below we will discuss the various approaches we used to derive data and meaning from the original txt files.

Discuss (in detail) the processes of

* Getting the data
* Cleaning/altering the data
* Visualizing the data
* Discussing and sharing results with group members

What you need to cover: Talk about every step in your process, talk about how and why you made the decisions that you did. Why did you pick the website that you did? Did you download every record, or just some? Did you download the data directly from the website, or did you use a tool like Webscraper.io? Did you use OpenRefine to change the raw data in some form, and if so, why did you change it the way you did? What program(s) or techniques did you decide on to visualize or analyze it? Why did you feel those programs (e.g. Tableau or Gephi) were better than other options, given your research interest? Did you have to do anything to the data inside Tableau?

To make elegant workflow charts, use [LucidChart](https://lucidchart.com)

Take plenty of screenshots to document your process...
    * **Mac** press 'command' + 'shift' + '4' and then drag a rectangle to take a screenshot of whatever you select... On
    * **Windows**, click the 'Start' button then type 'Snipping' and select Snipping Tool, then click 'New' and drag a rectangle to take a screenshot of whatever you select.
